{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/odemakinde/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "logdir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(400,input_shape=(9,), activation='relu'))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tune_param_classifier:\n",
    "    def __init__(self, x, y, epoch,learning_rate, no_of_layers, \n",
    "                 batch_size,validation_data):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.no_of_layers = no_of_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "        self.x_dimension = x.shape\n",
    "        self.target_dimension = y.shape\n",
    "        \n",
    "        #self layers range (start, stop, step)\n",
    "        self.layers_range = (100,1000,50)\n",
    "        \n",
    "    \n",
    "        #no of possiblities\n",
    "        self.possibilities = self.no_of_possibilities(len(self.layers_range))\n",
    "        \n",
    "        #call evaluate classifier to evaluate the network\n",
    "                \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def evaluate_classifier(self):\n",
    "        # evaluate network\n",
    "        a = self.x\n",
    "        b = self.y\n",
    "        c = self.epoch\n",
    "        d = self.no_of_layers\n",
    "        e = self.learning_rate\n",
    "        f = self.batch_size\n",
    "        g = self.validation_data\n",
    "        \n",
    "        dict_scores = self.evaluate_network(a,b,c,d,e,f,g)\n",
    "        \n",
    "        #find param with minimum variance\n",
    "        result = self.minimum_variance(dict_scores)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def minimum_variance(self, dict_scores):\n",
    "        variances = [np.var([np.var(m),np.var(n)]) for m,n in zip(dict_scores[i]['training_values'],\n",
    "                                                                 dict_scores[i]['testing_values']) \n",
    "                    for i in dict_scores.keys()]\n",
    "        result = {}\n",
    "        for key, value in zip(dict_scores.keys(),variances):  \n",
    "            if value == min(variances):\n",
    "                result = dict_scores[key]\n",
    "                break\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def no_of_posibilities(self, no_of_layers):\n",
    "        count = 1\n",
    "        for i in range(1, no_of_layers+1):\n",
    "            count *= i\n",
    "        return count\n",
    "    \n",
    "    \n",
    "    def create_network(self, neurons):\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # iterate through the dict layers\n",
    "        model.add(layers.Dense(neurons[0],input_shape=(self.x_dimension[1],), activation='relu'))\n",
    "        \n",
    "        # add hidden layers\n",
    "        for i in range(neurons[1:]):\n",
    "            model.add(layers.Dense(i, activation='relu'))\n",
    "            \n",
    "        # add output layers\n",
    "        if self.target_dimension[1] == None:\n",
    "            model.add(layers.Dense(1, activation = 'softmax'))\n",
    "        else:\n",
    "            model.add(self.target_dimension[1], activation = 'softmax')\n",
    "            \n",
    "        return model\n",
    "    \n",
    "   \n",
    "    \n",
    "    def categorical_compute(self, actual, prediction):\n",
    "        prediction = tf.argmax(logits, 1)\n",
    "        actual = tf.argmax(y,1)\n",
    "        \n",
    "        TP = tf.math.count_nonzero(prediction * actual)\n",
    "        TN = tf.math.count_nonzero((prediction - 1) * (actual - 1))\n",
    "        FP = tf.math.count_nonzero(prediction * (actual - 1))\n",
    "        FN = tf.math.count_nonzero((prediction - 1) * actual)\n",
    "        \n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "        Recall = TP/(TP+FN)\n",
    "        precision = TP/(TP+FP)\n",
    "        F1_Score = 2*(Recall * precision) / (Recall + precision)\n",
    "        return accuracy, f1_score, precision,recall\n",
    "\n",
    "    \n",
    "    \n",
    "    def binary_compute(self, actual, prediction):\n",
    "        accuracy = accuracy_score(actual, prediction)\n",
    "        f1_score = f1_score(actual, prediction)\n",
    "        precision = precision_score(actual, prediction)\n",
    "        recall = (precision *f1_score)/((2*precision)-f1_score)\n",
    "        return accuracy, f1_score, precision, recall\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate_network(self,x,y,epoch,no_of_layers,\n",
    "                         learning_rate,batch_size, \n",
    "                         validation_data):\n",
    "        # ranges of neuron values\n",
    "        data = [i for i in range(self.layers_range)]\n",
    "        \n",
    "        #optimizers\n",
    "        optimize = {\n",
    "            'rmsprop':tf.keras.optimizers.RMSprop(learning_rate),\n",
    "            'gradient descent':tf.keras.optimizers.SGD(learning_rate),\n",
    "            'adam':tf.keras.optimizers.Adam(learning_rate),\n",
    "            'adagrad':tf.keras.optimizers.AdaGrad(learning_rate),\n",
    "            'adadelta':tf.keras.optimizers.AdaDelta(learning_rate),\n",
    "            'nadem':tf.keras.optimizers.Nadem(learning_rate)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # losses\n",
    "        losses = {\n",
    "            'binary':tf.keras.losses.binary_crossentropy(from_logits = True),\n",
    "            'categorical':tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "        dict_neurons = {}\n",
    "        for pos in range(self.possibilities):\n",
    "            #randomly initialize values as numbers of neurons\n",
    "            current_neurons = np.random.choice(data, no_of_layers, replace = False)\n",
    "            \n",
    "            \n",
    "            #create network for the neurons\n",
    "            model = self.create_network(current_neurons)\n",
    "            \n",
    "            #Train the network on available optimizers and losses\n",
    "            for m in optimize.keys():\n",
    "                #document training params\n",
    "                dict_doc = {}\n",
    "                if (y.shape[1] in [None, 1]):\n",
    "                    model.compile(optimizer=optimize[m],\n",
    "                                  loss= losses['binary'],\n",
    "                                  metrics=['accuracy'])\n",
    "                    model.fit(x,y, epoch = epoch, batch_size= batch_size,\n",
    "                             validation_data = validation_data, verbose = 0)\n",
    "                    pred_train, pred_test = model.predict(x), model.predict(validation_data[0])\n",
    "                    \n",
    "                    train_result = self.binary_compute(y, pred_train)\n",
    "                    test_result = self.binary_compute(validation_data[1],pred_test)\n",
    "                    \n",
    "                    # document training params\n",
    "                    dict_doc['no_of_layers'] = no_of_layers\n",
    "                    dict_doc['neuron_values'] = current_neurons\n",
    "                    dict_doc['optimizer'] = m\n",
    "                    dict_doc['loss'] = 'binary_crossentropy'\n",
    "                    dict_doc['training_values'] = train_result\n",
    "                    dict_doc['testing_values'] = test_result\n",
    "                    \n",
    "                else:\n",
    "                    model.compile(optimizer = optimize[m],\n",
    "                                  loss= losses['categorical'],\n",
    "                                  metrics=['accuracy'])\n",
    "                    h = model.fit(x,y, epoch = epoch, batch_size= batch_size,\n",
    "                             validation_data = validation_data,verbose =0)\n",
    "                    pred_train, pred_test = model.predict(x), model.predict(validation_data[0])\n",
    "                    \n",
    "                    train_result = self.categorical_compute(y, pred_train)\n",
    "                    test_result = self.categorical_compute(validation_data[1],pred_test)\n",
    "                    \n",
    "                    # document training params\n",
    "                    dict_doc['no_of_layers'] = no_of_layers\n",
    "                    dict_doc['neuron_values'] = current_neurons\n",
    "                    dict_doc['optimizer'] = m\n",
    "                    dict_doc['loss'] = 'binary_crossentropy'\n",
    "                    dict_doc['training_values'] = train_result\n",
    "                    dict_doc['testing_values'] = test_result\n",
    "               \n",
    "                # take record of all iterations\n",
    "                dict_neurons[str(pos)+'_'+m] = dict_doc\n",
    "                    \n",
    "            \n",
    "        return dict_neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function var in module numpy:\n",
      "\n",
      "var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>)\n",
      "    Compute the variance along the specified axis.\n",
      "    \n",
      "    Returns the variance of the array elements, a measure of the spread of a\n",
      "    distribution.  The variance is computed for the flattened array by\n",
      "    default, otherwise over the specified axis.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Array containing numbers whose variance is desired.  If `a` is not an\n",
      "        array, a conversion is attempted.\n",
      "    axis : None or int or tuple of ints, optional\n",
      "        Axis or axes along which the variance is computed.  The default is to\n",
      "        compute the variance of the flattened array.\n",
      "    \n",
      "        .. versionadded:: 1.7.0\n",
      "    \n",
      "        If this is a tuple of ints, a variance is performed over multiple axes,\n",
      "        instead of a single axis or all the axes as before.\n",
      "    dtype : data-type, optional\n",
      "        Type to use in computing the variance.  For arrays of integer type\n",
      "        the default is `float64`; for arrays of float types it is the same as\n",
      "        the array type.\n",
      "    out : ndarray, optional\n",
      "        Alternate output array in which to place the result.  It must have\n",
      "        the same shape as the expected output, but the type is cast if\n",
      "        necessary.\n",
      "    ddof : int, optional\n",
      "        \"Delta Degrees of Freedom\": the divisor used in the calculation is\n",
      "        ``N - ddof``, where ``N`` represents the number of elements. By\n",
      "        default `ddof` is zero.\n",
      "    keepdims : bool, optional\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the input array.\n",
      "    \n",
      "        If the default value is passed, then `keepdims` will not be\n",
      "        passed through to the `var` method of sub-classes of\n",
      "        `ndarray`, however any non-default value will be.  If the\n",
      "        sub-class' method does not implement `keepdims` any\n",
      "        exceptions will be raised.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    variance : ndarray, see dtype parameter above\n",
      "        If ``out=None``, returns a new array containing the variance;\n",
      "        otherwise, a reference to the output array is returned.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    std, mean, nanmean, nanstd, nanvar\n",
      "    ufuncs-output-type\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The variance is the average of the squared deviations from the mean,\n",
      "    i.e.,  ``var = mean(abs(x - x.mean())**2)``.\n",
      "    \n",
      "    The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.\n",
      "    If, however, `ddof` is specified, the divisor ``N - ddof`` is used\n",
      "    instead.  In standard statistical practice, ``ddof=1`` provides an\n",
      "    unbiased estimator of the variance of a hypothetical infinite population.\n",
      "    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n",
      "    normally distributed variables.\n",
      "    \n",
      "    Note that for complex numbers, the absolute value is taken before\n",
      "    squaring, so that the result is always real and nonnegative.\n",
      "    \n",
      "    For floating-point input, the variance is computed using the same\n",
      "    precision the input has.  Depending on the input data, this can cause\n",
      "    the results to be inaccurate, especially for `float32` (see example\n",
      "    below).  Specifying a higher-accuracy accumulator using the ``dtype``\n",
      "    keyword can alleviate this issue.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.array([[1, 2], [3, 4]])\n",
      "    >>> np.var(a)\n",
      "    1.25\n",
      "    >>> np.var(a, axis=0)\n",
      "    array([1.,  1.])\n",
      "    >>> np.var(a, axis=1)\n",
      "    array([0.25,  0.25])\n",
      "    \n",
      "    In single precision, var() can be inaccurate:\n",
      "    \n",
      "    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n",
      "    >>> a[0, :] = 1.0\n",
      "    >>> a[1, :] = 0.1\n",
      "    >>> np.var(a)\n",
      "    0.20250003\n",
      "    \n",
      "    Computing the variance in float64 is more accurate:\n",
      "    \n",
      "    >>> np.var(a, dtype=np.float64)\n",
      "    0.20249999932944759 # may vary\n",
      "    >>> ((1-0.55)**2 + (0.1-0.55)**2)/2\n",
      "    0.2025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class tune_param_classifier:\n",
    "    def __init__(self, x, y, epoch,learning_rate, no_of_layers, \n",
    "                 batch_size,validation_data):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.no_of_layers = no_of_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "        self.x_dimension = x.shape\n",
    "        self.target_dimension = y.shape\n",
    "        \n",
    "        #self layers range (start, stop, step)\n",
    "        self.layers_range = (100,1000,50)\n",
    "        \n",
    "    \n",
    "        #no of possiblities\n",
    "        self.possibilities = self.no_of_possibilities(len(self.layers_range))\n",
    "        \n",
    "        #call evaluate classifier to evaluate the network\n",
    "                \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def evaluate_classifier(self):\n",
    "        # evaluate network\n",
    "        a = self.x\n",
    "        b = self.y\n",
    "        c = self.epoch\n",
    "        d = self.no_of_layers\n",
    "        e = self.learning_rate\n",
    "        f = self.batch_size\n",
    "        g = self.validation_data\n",
    "        \n",
    "        dict_scores = self.evaluate_network(a,b,c,d,e,f,g)\n",
    "        \n",
    "        #find param with minimum variance\n",
    "        result = self.minimum_variance(dict_scores)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def minimum_variance(self, dict_scores):\n",
    "        variances = [np.var([np.var(m),np.var(n)]) for m,n in zip(dict_scores[i]['training_values'],\n",
    "                                                                 dict_scores[i]['testing_values']) \n",
    "                    for i in dict_scores.keys()]\n",
    "        result = {}\n",
    "        for key, value in zip(dict_scores.keys(),variances):  \n",
    "            if value == min(variances):\n",
    "                result = dict_scores[key]\n",
    "                break\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def no_of_posibilities(self, no_of_layers):\n",
    "        count = 1\n",
    "        for i in range(1, no_of_layers+1):\n",
    "            count *= i\n",
    "        return count\n",
    "    \n",
    "    \n",
    "    def create_network(self, neurons):\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # iterate through the dict layers\n",
    "        model.add(layers.Dense(neurons[0],input_shape=(self.x_dimension[1],), activation='relu'))\n",
    "        \n",
    "        # add hidden layers\n",
    "        for i in range(neurons[1:]):\n",
    "            model.add(layers.Dense(i, activation='relu'))\n",
    "            \n",
    "        # add output layers\n",
    "        if self.target_dimension[1] == None:\n",
    "            model.add(layers.Dense(1, activation = 'softmax'))\n",
    "        else:\n",
    "            model.add(self.target_dimension[1], activation = 'softmax')\n",
    "            \n",
    "        return model\n",
    "    \n",
    "   \n",
    "    \n",
    "    def _compute(self, actual, prediction):\n",
    "        prediction = tf.argmax(logits, 1)\n",
    "        actual = tf.argmax(y,1)\n",
    "        \n",
    "        TP = tf.math.count_nonzero(prediction * actual)\n",
    "        TN = tf.math.count_nonzero((prediction - 1) * (actual - 1))\n",
    "        FP = tf.math.count_nonzero(prediction * (actual - 1))\n",
    "        FN = tf.math.count_nonzero((prediction - 1) * actual)\n",
    "        \n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "        Recall = TP/(TP+FN)\n",
    "        precision = TP/(TP+FP)\n",
    "        F1_Score = 2*(Recall * precision) / (Recall + precision)\n",
    "        return accuracy, f1_score, precision,recall\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate_network(self,x,y,epoch,no_of_layers,\n",
    "                         learning_rate,batch_size, \n",
    "                         validation_data):\n",
    "        # ranges of neuron values\n",
    "        data = [i for i in range(self.layers_range)]\n",
    "        \n",
    "        #optimizers\n",
    "        optimize = {\n",
    "            'rmsprop':tf.keras.optimizers.RMSprop(learning_rate),\n",
    "            'gradient descent':tf.keras.optimizers.SGD(learning_rate),\n",
    "            'adam':tf.keras.optimizers.Adam(learning_rate),\n",
    "            'adagrad':tf.keras.optimizers.AdaGrad(learning_rate),\n",
    "            'adadelta':tf.keras.optimizers.AdaDelta(learning_rate),\n",
    "            'nadem':tf.keras.optimizers.Nadem(learning_rate)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # losses\n",
    "        losses = {\n",
    "            'mse':tf.keras.losses.MSE,\n",
    "            'mae':tf.keras.losses.MAE,\n",
    "            'mape':tf.keras.losses.MAPE,\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "        dict_neurons = {}\n",
    "        for pos in range(self.possibilities):\n",
    "            #randomly initialize values as numbers of neurons\n",
    "            current_neurons = np.random.choice(data, no_of_layers, replace = False)\n",
    "            \n",
    "            \n",
    "            #create network for the neurons\n",
    "            model = self.create_network(current_neurons)\n",
    "            \n",
    "            #Train the network on available optimizers and losses\n",
    "            for m in optimize.keys():\n",
    "                #document training params\n",
    "                dict_doc = {}\n",
    "                model.compile(optimizer=optimize[m],\n",
    "                              loss= losses['mae'],\n",
    "                              metrics=['accuracy'])\n",
    "                model.fit(x,y, epoch = epoch, batch_size= batch_size,\n",
    "                         validation_data = validation_data, verbose = 0)\n",
    "                pred_train, pred_test = model.predict(x), model.predict(validation_data[0])\n",
    "\n",
    "                train_result = self._compute(y, pred_train)\n",
    "                test_result = self._compute(validation_data[1],pred_test)\n",
    "\n",
    "                # document training params\n",
    "                dict_doc['no_of_layers'] = no_of_layers\n",
    "                dict_doc['neuron_values'] = current_neurons\n",
    "                dict_doc['optimizer'] = m\n",
    "                dict_doc['loss'] = 'mean squared error'\n",
    "                dict_doc['training_values'] = train_result\n",
    "                dict_doc['testing_values'] = test_result\n",
    "                  \n",
    "                # take record of all iterations\n",
    "                dict_neurons[str(pos)+'_'+m] = dict_doc\n",
    "                    \n",
    "            \n",
    "        return dict_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var([np.var([0.55,0.34,0.45,0.90]),np.var([0.55,0.34,0.45,0.90])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
