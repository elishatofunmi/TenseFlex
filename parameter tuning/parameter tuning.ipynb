{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/odemakinde/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "logdir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(400,input_shape=(9,), activation='relu'))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tune_param_classifier:\n",
    "    def __init__(self, x, y, epoch,learning_rate, no_of_layers, \n",
    "                 batch_size,validation_data):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.no_of_layers = no_of_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "        self.x_dimension = x.shape\n",
    "        self.target_dimension = y.shape\n",
    "        \n",
    "        #self layers range (start, stop, step)\n",
    "        self.layers_range = (100,1000,50)\n",
    "        \n",
    "    \n",
    "        #no of possiblities\n",
    "        self.possibilities = self.no_of_possibilities(len(self.layers_range))\n",
    "        \n",
    "        #call evaluate classifier to evaluate the network\n",
    "                \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def evaluate_classifier(self):\n",
    "        # evaluate network\n",
    "        a = self.x\n",
    "        b = self.y\n",
    "        c = self.epoch\n",
    "        d = self.no_of_layers\n",
    "        e = self.learning_rate\n",
    "        f = self.batch_size\n",
    "        g = self.validation_data\n",
    "        \n",
    "        dict_scores = self.evaluate_network(a,b,c,d,e,f,g)\n",
    "        \n",
    "        #find param with minimum variance\n",
    "        result = self.minimum_variance(dict_scores)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def minimum_variance(self, dict_scores):\n",
    "        variances = [np.var([np.var(m),np.var(n)]) for m,n in zip(dict_scores[i]['training_values'],\n",
    "                                                                 dict_scores[i]['testing_values']) \n",
    "                    for i in dict_scores.keys()]\n",
    "        result = {}\n",
    "        for key, value in zip(dict_scores.keys(),variances):  \n",
    "            if value == min(variances):\n",
    "                result = dict_scores[key]\n",
    "                break\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def no_of_posibilities(self, no_of_layers):\n",
    "        count = 1\n",
    "        for i in range(1, no_of_layers+1):\n",
    "            count *= i\n",
    "        return count\n",
    "    \n",
    "    \n",
    "    def create_network(self, neurons):\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # iterate through the dict layers\n",
    "        model.add(layers.Dense(neurons[0],input_shape=(self.x_dimension[1],), activation='relu'))\n",
    "        \n",
    "        # add hidden layers\n",
    "        for i in range(neurons[1:]):\n",
    "            model.add(layers.Dense(i, activation='relu'))\n",
    "            \n",
    "        # add output layers\n",
    "        if self.target_dimension[1] == None:\n",
    "            model.add(layers.Dense(1, activation = 'softmax'))\n",
    "        else:\n",
    "            model.add(self.target_dimension[1], activation = 'softmax')\n",
    "            \n",
    "        return model\n",
    "    \n",
    "   \n",
    "    \n",
    "    def categorical_compute(self, actual, prediction):\n",
    "        prediction = tf.argmax(logits, 1)\n",
    "        actual = tf.argmax(y,1)\n",
    "        \n",
    "        TP = tf.math.count_nonzero(prediction * actual)\n",
    "        TN = tf.math.count_nonzero((prediction - 1) * (actual - 1))\n",
    "        FP = tf.math.count_nonzero(prediction * (actual - 1))\n",
    "        FN = tf.math.count_nonzero((prediction - 1) * actual)\n",
    "        \n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "        Recall = TP/(TP+FN)\n",
    "        precision = TP/(TP+FP)\n",
    "        F1_Score = 2*(Recall * precision) / (Recall + precision)\n",
    "        return accuracy, f1_score, precision,recall\n",
    "\n",
    "    \n",
    "    \n",
    "    def binary_compute(self, actual, prediction):\n",
    "        accuracy = accuracy_score(actual, prediction)\n",
    "        f1_score = f1_score(actual, prediction)\n",
    "        precision = precision_score(actual, prediction)\n",
    "        recall = (precision *f1_score)/((2*precision)-f1_score)\n",
    "        return accuracy, f1_score, precision, recall\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate_network(self,x,y,epoch,no_of_layers,\n",
    "                         learning_rate,batch_size, \n",
    "                         validation_data):\n",
    "        # ranges of neuron values\n",
    "        data = [i for i in range(self.layers_range)]\n",
    "        \n",
    "        #optimizers\n",
    "        optimize = {\n",
    "            'rmsprop':tf.keras.optimizers.RMSprop(learning_rate),\n",
    "            'gradient descent':tf.keras.optimizers.SGD(learning_rate),\n",
    "            'adam':tf.keras.optimizers.Adam(learning_rate),\n",
    "            'adagrad':tf.keras.optimizers.AdaGrad(learning_rate),\n",
    "            'adadelta':tf.keras.optimizers.AdaDelta(learning_rate),\n",
    "            'nadem':tf.keras.optimizers.Nadem(learning_rate)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # losses\n",
    "        losses = {\n",
    "            'binary':tf.keras.losses.binary_crossentropy(from_logits = True),\n",
    "            'categorical':tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "        dict_neurons = {}\n",
    "        for pos in range(self.possibilities):\n",
    "            #randomly initialize values as numbers of neurons\n",
    "            current_neurons = np.random.choice(data, no_of_layers, replace = False)\n",
    "            \n",
    "            \n",
    "            #create network for the neurons\n",
    "            model = self.create_network(current_neurons)\n",
    "            \n",
    "            #Train the network on available optimizers and losses\n",
    "            for m in optimize.keys():\n",
    "                #document training params\n",
    "                dict_doc = {}\n",
    "                if (y.shape[1] in [None, 1]):\n",
    "                    model.compile(optimizer=optimize[m],\n",
    "                                  loss= losses['binary'],\n",
    "                                  metrics=['accuracy'])\n",
    "                    model.fit(x,y, epoch = epoch, batch_size= batch_size,\n",
    "                             validation_data = validation_data, verbose = 0)\n",
    "                    pred_train, pred_test = model.predict(x), model.predict(validation_data[0])\n",
    "                    \n",
    "                    train_result = self.binary_compute(y, pred_train)\n",
    "                    test_result = self.binary_compute(validation_data[1],pred_test)\n",
    "                    \n",
    "                    # document training params\n",
    "                    dict_doc['no_of_layers'] = no_of_layers\n",
    "                    dict_doc['neuron_values'] = current_neurons\n",
    "                    dict_doc['optimizer'] = m\n",
    "                    dict_doc['loss'] = 'binary_crossentropy'\n",
    "                    dict_doc['training_values'] = train_result\n",
    "                    dict_doc['testing_values'] = test_result\n",
    "                    \n",
    "                else:\n",
    "                    model.compile(optimizer = optimize[m],\n",
    "                                  loss= losses['categorical'],\n",
    "                                  metrics=['accuracy'])\n",
    "                    h = model.fit(x,y, epoch = epoch, batch_size= batch_size,\n",
    "                             validation_data = validation_data,verbose =0)\n",
    "                    pred_train, pred_test = model.predict(x), model.predict(validation_data[0])\n",
    "                    \n",
    "                    train_result = self.categorical_compute(y, pred_train)\n",
    "                    test_result = self.categorical_compute(validation_data[1],pred_test)\n",
    "                    \n",
    "                    # document training params\n",
    "                    dict_doc['no_of_layers'] = no_of_layers\n",
    "                    dict_doc['neuron_values'] = current_neurons\n",
    "                    dict_doc['optimizer'] = m\n",
    "                    dict_doc['loss'] = 'binary_crossentropy'\n",
    "                    dict_doc['training_values'] = train_result\n",
    "                    dict_doc['testing_values'] = test_result\n",
    "               \n",
    "                # take record of all iterations\n",
    "                dict_neurons[str(pos)+'_'+m] = dict_doc\n",
    "                    \n",
    "            \n",
    "        return dict_neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tune_param_classifier:\n",
    "    def __init__(self, x, y, epoch,learning_rate, no_of_layers, \n",
    "                 batch_size,validation_data):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.no_of_layers = no_of_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "        self.x_dimension = x.shape\n",
    "        self.target_dimension = y.shape\n",
    "        \n",
    "        #self layers range (start, stop, step)\n",
    "        self.layers_range = (100,1000,50)\n",
    "        \n",
    "    \n",
    "        #no of possiblities\n",
    "        self.possibilities = self.no_of_possibilities(len(self.layers_range))\n",
    "        \n",
    "        #call evaluate classifier to evaluate the network\n",
    "                \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def evaluate_classifier(self):\n",
    "        # evaluate network\n",
    "        a = self.x\n",
    "        b = self.y\n",
    "        c = self.epoch\n",
    "        d = self.no_of_layers\n",
    "        e = self.learning_rate\n",
    "        f = self.batch_size\n",
    "        g = self.validation_data\n",
    "        \n",
    "        dict_scores = self.evaluate_network(a,b,c,d,e,f,g)\n",
    "        \n",
    "        #find param with minimum variance\n",
    "        result = self.minimum_variance(dict_scores)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def minimum_variance(self, dict_scores):\n",
    "        variances = [np.var([np.var(m),np.var(n)]) for m,n in zip(dict_scores[i]['training_values'],\n",
    "                                                                 dict_scores[i]['testing_values']) \n",
    "                    for i in dict_scores.keys()]\n",
    "        result = {}\n",
    "        for key, value in zip(dict_scores.keys(),variances):  \n",
    "            if value == min(variances):\n",
    "                result = dict_scores[key]\n",
    "                break\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def no_of_posibilities(self, no_of_layers):\n",
    "        count = 1\n",
    "        for i in range(1, no_of_layers+1):\n",
    "            count *= i\n",
    "        return count\n",
    "    \n",
    "    \n",
    "    def create_network(self, neurons):\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # iterate through the dict layers\n",
    "        model.add(layers.Dense(neurons[0],input_shape=(self.x_dimension[1],), activation='relu'))\n",
    "        \n",
    "        # add hidden layers\n",
    "        for i in range(neurons[1:]):\n",
    "            model.add(layers.Dense(i, activation='relu'))\n",
    "            \n",
    "        # add output layers\n",
    "        if self.target_dimension[1] == None:\n",
    "            model.add(layers.Dense(1, activation = 'softmax'))\n",
    "        else:\n",
    "            model.add(self.target_dimension[1], activation = 'softmax')\n",
    "            \n",
    "        return model\n",
    "    \n",
    "   \n",
    "    \n",
    "    def _compute(self, actual, prediction):\n",
    "        prediction = tf.argmax(logits, 1)\n",
    "        actual = tf.argmax(y,1)\n",
    "        \n",
    "        TP = tf.math.count_nonzero(prediction * actual)\n",
    "        TN = tf.math.count_nonzero((prediction - 1) * (actual - 1))\n",
    "        FP = tf.math.count_nonzero(prediction * (actual - 1))\n",
    "        FN = tf.math.count_nonzero((prediction - 1) * actual)\n",
    "        \n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "        Recall = TP/(TP+FN)\n",
    "        precision = TP/(TP+FP)\n",
    "        F1_Score = 2*(Recall * precision) / (Recall + precision)\n",
    "        return accuracy, f1_score, precision,recall\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate_network(self,x,y,epoch,no_of_layers,\n",
    "                         learning_rate,batch_size, \n",
    "                         validation_data):\n",
    "        # ranges of neuron values\n",
    "        data = [i for i in range(self.layers_range)]\n",
    "        \n",
    "        #optimizers\n",
    "        optimize = {\n",
    "            'rmsprop':tf.keras.optimizers.RMSprop(learning_rate),\n",
    "            'gradient descent':tf.keras.optimizers.SGD(learning_rate),\n",
    "            'adam':tf.keras.optimizers.Adam(learning_rate),\n",
    "            'adagrad':tf.keras.optimizers.AdaGrad(learning_rate),\n",
    "            'adadelta':tf.keras.optimizers.AdaDelta(learning_rate),\n",
    "            'nadem':tf.keras.optimizers.Nadem(learning_rate)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # losses\n",
    "        losses = {\n",
    "            'mse':tf.keras.losses.MSE,\n",
    "            'mae':tf.keras.losses.MAE,\n",
    "            'mape':tf.keras.losses.MAPE,\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "        dict_neurons = {}\n",
    "        for pos in range(self.possibilities):\n",
    "            #randomly initialize values as numbers of neurons\n",
    "            current_neurons = np.random.choice(data, no_of_layers, replace = False)\n",
    "            \n",
    "            \n",
    "            #create network for the neurons\n",
    "            model = self.create_network(current_neurons)\n",
    "            \n",
    "            #Train the network on available optimizers and losses\n",
    "            for m in optimize.keys():\n",
    "                #document training params\n",
    "                dict_doc = {}\n",
    "                model.compile(optimizer=optimize[m],\n",
    "                              loss= losses['mae'],\n",
    "                              metrics=['accuracy'])\n",
    "                model.fit(x,y, epoch = epoch, batch_size= batch_size,\n",
    "                         validation_data = validation_data, verbose = 0)\n",
    "                pred_train, pred_test = model.predict(x), model.predict(validation_data[0])\n",
    "\n",
    "                train_result = self._compute(y, pred_train)\n",
    "                test_result = self._compute(validation_data[1],pred_test)\n",
    "\n",
    "                # document training params\n",
    "                dict_doc['no_of_layers'] = no_of_layers\n",
    "                dict_doc['neuron_values'] = current_neurons\n",
    "                dict_doc['optimizer'] = m\n",
    "                dict_doc['loss'] = 'mean squared error'\n",
    "                dict_doc['training_values'] = train_result\n",
    "                dict_doc['testing_values'] = test_result\n",
    "                  \n",
    "                # take record of all iterations\n",
    "                dict_neurons[str(pos)+'_'+m] = dict_doc\n",
    "                    \n",
    "            \n",
    "        return dict_neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "x, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__pycache__',\n",
       " 'TenseFlexRegressor.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'parameter tuning.ipynb',\n",
       " 'TenseFlexClassiffier.py']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run TenseFlexClassiffier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load TenseFlexClassiffier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42, stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def one_hot(y):\n",
    "    frame = pd.DataFrame({'k':y.astype(np.float32)})\n",
    "    return pd.get_dummies(frame, columns = ['k'])\n",
    "\n",
    "y_train = one_hot(y_train)\n",
    "y_test = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TenseFlexClassiffier import tune_param_classifier as tm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "learning_rate = 0.01\n",
    "no_of_layers = 2\n",
    "batch_size = 10\n",
    "validation_data = (x_test, y_test)\n",
    "flex_tense = tm(x_train, y_train, epoch,learning_rate, no_of_layers, \n",
    "                 batch_size,validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.keras.api._v1.keras.optimizers' has no attribute 'AdaGrad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4e62a119c9f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflex_tense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/model parameter hypertuning/TenseFlex/parameter tuning/TenseFlexClassiffier.py\u001b[0m in \u001b[0;36mevaluate_classifier\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mdict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#find param with minimum variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/model parameter hypertuning/TenseFlex/parameter tuning/TenseFlexClassiffier.py\u001b[0m in \u001b[0;36mevaluate_network\u001b[0;34m(self, x, y, epoch, no_of_layers, learning_rate, batch_size, validation_data)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;34m'gradient descent'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;34m'adam'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;34m'adagrad'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0;34m'adadelta'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaDelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;34m'nadem'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNadem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    167\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v1.keras.optimizers' has no attribute 'AdaGrad'"
     ]
    }
   ],
   "source": [
    "flex_tense.evaluate_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
