{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/odemakinde/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "logdir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(400,input_shape=(9,), activation='relu'))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tune_param_classifier:\n",
    "    def __init__(self, x, y, epoch,learning_rate, no_of_layers, \n",
    "                 batch_size, metric = 'accuracy'):\n",
    "        self.x_dimension = x.shape\n",
    "        self.target_dimension = y.shape\n",
    "        \n",
    "        #optimizer variation\n",
    "        self.optimizers = []\n",
    "        \n",
    "        #self layers range (start, stop, step)\n",
    "        self.layers_range = (100,1000,50)\n",
    "        \n",
    "    \n",
    "        #no of possiblities\n",
    "        self.possibilities = self.no_of_possibilities(len(self.layers_range))\n",
    "        \n",
    "        # evaluate network\n",
    "        self.evaluate_network(x,y,epoch,no_of_layers,learning_rate,batch_size,metric)\n",
    "                \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def no_of_posibilities(self, no_of_layers):\n",
    "        count = 1\n",
    "        for i in range(1, no_of_layers+1):\n",
    "            count *= i\n",
    "        return count\n",
    "    \n",
    "    \n",
    "    def create_network(self, neurons):\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # iterate through the dict layers\n",
    "        model.add(layers.Dense(neurons[0],input_shape=(self.x_dimension[1],), activation='relu'))\n",
    "        \n",
    "        # add hidden layers\n",
    "        for i in range(neurons[1:]):\n",
    "            model.add(layers.Dense(i, activation='relu'))\n",
    "            \n",
    "        # add output layers\n",
    "        if self.target_dimension[1] == None:\n",
    "            model.add(layers.Dense(1, activation = 'softmax'))\n",
    "        else:\n",
    "            model.add(self.target_dimension[1], activation = 'softmax')\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate_network(self,x,y,epoch,no_of_layers,\n",
    "                         learning_rate,batch_size,metric):\n",
    "        # ranges of neuron values\n",
    "        data = [i for i in range(self.layers_range)]\n",
    "        \n",
    "        #optimizers\n",
    "        optimizer = ['rmsprop','gradient descent','adam', 'adagrad','adadelta','nadem']\n",
    "        \n",
    "\n",
    "        dict_neurons = {}\n",
    "        for i,pos in enumerate(range(self.possibilities)):\n",
    "            current_neurons = np.random.choice(data, no_of_layers, replace = False)\n",
    "            # take record of all neurons\n",
    "            dict_neurons[str(i)] = current_neurons\n",
    "            \n",
    "            #create network for the neurons\n",
    "            model = self.create_network(current_neurons)\n",
    "            \n",
    "            for m in optimizer:\n",
    "                if m == 'rmsprop' and y.shape[1] in [None, 1]:\n",
    "                    model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\n",
    "                                  loss= tf.keras.losses.binary_crossentropy(0.01),\n",
    "                                  metrics=['accuracy'])\n",
    "                elif m== 'rmsprop' and y.shape[1]>1:\n",
    "                    model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\n",
    "                                  loss= tf.keras.losses.categorical_crossentropy(logit = True),\n",
    "                                  metrics=['accuracy'])\n",
    "                elif m== 'adam' and y.shape[1] in [None, 1]:\n",
    "                    model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "                                  loss= tf.keras.losses.binary_crossentropy(0.01),\n",
    "                                  metrics=['accuracy'])\n",
    "                elif m== 'adam' and y.shape[1]>1:\n",
    "                    model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "                                  loss= tf.keras.losses.categorical_crossentropy(logit = True),\n",
    "                                  metrics=['accuracy'])\n",
    "                    \n",
    "                elif m== 'gradient descent' and y.shape[1] in [None, 1]:\n",
    "                    model.compile(optimizer=tf.keras.optimizers.SGD(0.01),\n",
    "                                  loss= tf.keras.losses.binary_crossentropy(),\n",
    "                                  metrics=['accuracy'])\n",
    "                elif m== 'gradient descent' and y.shape[1]>1:\n",
    "                    model.compile(optimizer=tf.keras.optimizers.SGD(0.01),\n",
    "                                  loss= tf.keras.losses.categorical_crossentropy(logit = True),\n",
    "                                  metrics=['accuracy'])\n",
    "                    \n",
    "                    \n",
    "            \n",
    "        return \n",
    "    \n",
    "    \n",
    "    \n",
    "tf.keras.optimizers.G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possibilities(dict_array):\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
