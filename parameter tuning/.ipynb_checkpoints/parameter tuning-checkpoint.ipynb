{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/odemakinde/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "logdir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(400,input_shape=(9,), activation='relu'))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tune_param_classifier:\n",
    "    def __init__(self, x, y, epoch,learning_rate, no_of_layers, \n",
    "                 batch_size,validation_data, metric = 'accuracy'):\n",
    "        self.x_dimension = x.shape\n",
    "        self.target_dimension = y.shape\n",
    "        \n",
    "        #self layers range (start, stop, step)\n",
    "        self.layers_range = (100,1000,50)\n",
    "        \n",
    "    \n",
    "        #no of possiblities\n",
    "        self.possibilities = self.no_of_possibilities(len(self.layers_range))\n",
    "        \n",
    "        # evaluate network\n",
    "        self.evaluate_network(x,y,epoch,no_of_layers,learning_rate,\n",
    "                              batch_size,metric, validation_data)\n",
    "                \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def no_of_posibilities(self, no_of_layers):\n",
    "        count = 1\n",
    "        for i in range(1, no_of_layers+1):\n",
    "            count *= i\n",
    "        return count\n",
    "    \n",
    "    \n",
    "    def create_network(self, neurons):\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        # iterate through the dict layers\n",
    "        model.add(layers.Dense(neurons[0],input_shape=(self.x_dimension[1],), activation='relu'))\n",
    "        \n",
    "        # add hidden layers\n",
    "        for i in range(neurons[1:]):\n",
    "            model.add(layers.Dense(i, activation='relu'))\n",
    "            \n",
    "        # add output layers\n",
    "        if self.target_dimension[1] == None:\n",
    "            model.add(layers.Dense(1, activation = 'softmax'))\n",
    "        else:\n",
    "            model.add(self.target_dimension[1], activation = 'softmax')\n",
    "            \n",
    "        return model\n",
    "    \n",
    "   \n",
    "    \n",
    "    def categorical_compute(self, actual, prediction):\n",
    "        prediction = tf.argmax(logits, 1)\n",
    "        actual = tf.argmax(y,1)\n",
    "        \n",
    "        TP = tf.math.count_nonzero(prediction * actual)\n",
    "        TN = tf.math.count_nonzero((prediction - 1) * (actual - 1))\n",
    "        FP = tf.math.count_nonzero(prediction * (actual - 1))\n",
    "        FN = tf.math.count_nonzero((prediction - 1) * actual)\n",
    "        \n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "        Recall = TP/(TP+FN)\n",
    "        precision = TP/(TP+FP)\n",
    "        F1_Score = 2*(Recall * precision) / (Recall + precision)\n",
    "        return accuracy, f1_score, precision,recall\n",
    "\n",
    "    \n",
    "    \n",
    "    def binary_compute(self, actual, prediction):\n",
    "        accuracy = accuracy_score(actual, prediction)\n",
    "        f1_score = f1_score(actual, prediction)\n",
    "        precision = precision_score(actual, prediction)\n",
    "        recall = (precision *f1_score)/((2*precision)-f1_score)\n",
    "        return accuracy, f1_score, precision, recall\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate_network(self,x,y,epoch,no_of_layers,\n",
    "                         learning_rate,batch_size,metric, \n",
    "                         validation_data):\n",
    "        # ranges of neuron values\n",
    "        data = [i for i in range(self.layers_range)]\n",
    "        \n",
    "        #optimizers\n",
    "        optimize = {\n",
    "            'rmsprop':tf.keras.optimizers.RMSprop(learning_rate),\n",
    "            'gradient descent':tf.keras.optimizers.SGD(learning_rate),\n",
    "            'adam':tf.keras.optimizers.Adam(learning_rate),\n",
    "            'adagrad':tf.keras.optimizers.AdaGrad(learning_rate),\n",
    "            'adadelta':tf.keras.optimizers.AdaDelta(learning_rate),\n",
    "            'nadem':tf.keras.optimizers.Nadem(learning_rate)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # losses\n",
    "        losses = {\n",
    "            'binary':tf.keras.losses.binary_crossentropy(0.01),\n",
    "            'categorical':tf.keras.losses.categorical_crossentropy(logit = True)\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "        dict_neurons = {}\n",
    "        for pos in range(self.possibilities):\n",
    "            #randomly initialize values as numbers of neurons\n",
    "            current_neurons = np.random.choice(data, no_of_layers, replace = False)\n",
    "            \n",
    "            \n",
    "            #create network for the neurons\n",
    "            model = self.create_network(current_neurons)\n",
    "            \n",
    "            #Train the network on available optimizers and losses\n",
    "            for m in optimize.keys():\n",
    "                #document training params\n",
    "                dict_doc = {}\n",
    "                if (y.shape[1] in [None, 1]):\n",
    "                    model.compile(optimizer=optimize[m],\n",
    "                                  loss= losses['binary'],\n",
    "                                  metrics=['accuracy'])\n",
    "                    model.fit(x,y, epoch = epoch, batch_size= batch_size,\n",
    "                             validation_data = validation_data, verbose = 0)\n",
    "                    pred_train, pred_test = model.predict(x), model.predict(validation_data[0])\n",
    "                    \n",
    "                    train_result = self.binary_compute(y, pred_train)\n",
    "                    test_result = self.categorical_compute(validation_data[1],pred_test)\n",
    "                    \n",
    "                    # document training params\n",
    "                    dict_doc['no_of_layers'] = no_of_layers\n",
    "                    dict_doc['neuron_values'] = current_neurons\n",
    "                    dict_doc['optimizer'] = m\n",
    "                    dict_doc['loss'] = 'binary_crossentropy'\n",
    "                    dict_doc['training_values'] = train_result\n",
    "                    dict_doc['testing_values'] = test_result\n",
    "                    \n",
    "                else:\n",
    "                    model.compile(optimizer = optimize[m],\n",
    "                                  loss= losses['categorical'],\n",
    "                                  metrics=['accuracy'])\n",
    "                    h = model.fit(x,y, epoch = epoch, batch_size= batch_size,\n",
    "                             validation_data = validation_data,verbose =0)\n",
    "                    pred_train, pred_test = model.predict(x), model.predict(validation_data[0])\n",
    "                    \n",
    "                    train_result = self.binary_compute(y, pred_train)\n",
    "                    test_result = self.categorical_compute(validation_data[1],pred_test)\n",
    "                    \n",
    "                    # document training params\n",
    "                    dict_doc['no_of_layers'] = no_of_layers\n",
    "                    dict_doc['neuron_values'] = current_neurons\n",
    "                    dict_doc['optimizer'] = m\n",
    "                    dict_doc['loss'] = 'binary_crossentropy'\n",
    "                    dict_doc['training_values'] = train_result\n",
    "                    dict_doc['testing_values'] = test_result\n",
    "               \n",
    "                # take record of all iterations\n",
    "                dict_neurons[str(pos)+'_'+m] = dict_doc\n",
    "                    \n",
    "            \n",
    "        return dict_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possibilities(dict_array):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
